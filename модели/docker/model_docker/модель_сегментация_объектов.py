# -*- coding: utf-8 -*-
"""–ö–æ–ø–∏—è –±–ª–æ–∫–Ω–æ—Ç–∞ "–º–æ–¥–µ–ª—å_—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è-–æ–±—ä–µ–∫—Ç–æ–≤.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wDJgRiJFV4FLOuciwIj5971v9Sb93DfJ

# 2 –≤–µ—Ä—Å–∏—è
"""

from google.colab import drive
drive.mount('/content/drive')

import os
from glob import glob

TRAIN_IMG_DIR = "/content/drive/MyDrive/data/train/image"
TRAIN_MASK_DIR = "/content/drive/MyDrive/data/train/label"

VAL_IMG_DIR = "/content/drive/MyDrive/data/val/image"
VAL_MASK_DIR = "/content/drive/MyDrive/data/val/label"

print("Train images:", len(glob(TRAIN_IMG_DIR + "/*.npy")))
print("Train masks:", len(glob(TRAIN_MASK_DIR + "/*.npy")))
print("Val images:", len(glob(VAL_IMG_DIR + "/*.npy")))
print("Val masks:", len(glob(VAL_MASK_DIR + "/*.npy")))

# –ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
print("Train images sample:", glob(TRAIN_IMG_DIR + "/*.npy")[:5])
print("Train masks sample:", glob(TRAIN_MASK_DIR + "/*.npy")[:5])

import torch
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
from torch.utils.data import DataLoader
from glob import glob
import os
from tqdm import tqdm
import segmentation_models_pytorch as smp

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
BATCH_SIZE = 4
NUM_CLASSES = 19
LR = 1e-3
EPOCHS = 25

"""–®–∞–≥ 4. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""

train_transform = A.Compose([
    A.Resize(256, 512),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    ToTensorV2(),
])

val_transform = A.Compose([
    A.Resize(256, 512),
    ToTensorV2(),
])

class SegmentationDataset(torch.utils.data.Dataset):
    def __init__(self, image_dir, mask_dir, transform):
        self.image_paths = sorted(glob(os.path.join(image_dir, '*.npy')))
        self.mask_paths = sorted(glob(os.path.join(mask_dir, '*.npy')))
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = np.load(self.image_paths[idx]).astype(np.uint8)
        mask = np.load(self.mask_paths[idx]).astype(np.uint8)
        mask[mask == 255] = 0  # üõ†Ô∏è –∑–∞–º–µ–Ω—è–µ–º "ignore" –Ω–∞ —Ñ–æ–Ω

        augmented = self.transform(image=image, mask=mask)
        image = augmented['image'].float() / 255.0  # üîÑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [0,1]
        mask = augmented['mask']

        return image, mask.long()

"""–®–∞–≥ 5. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–æ–≤"""

train_dataset = SegmentationDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, train_transform)
val_dataset = SegmentationDataset(VAL_IMG_DIR, VAL_MASK_DIR, val_transform)

print("Train dataset size:", len(train_dataset))
print("Validation dataset size:", len(val_dataset))

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=0)

"""–®–∞–≥ 6. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ lr_scheduler"""

model = smp.Unet(encoder_name="resnet34", encoder_weights="imagenet", classes=NUM_CLASSES, activation=None).to(DEVICE)
loss_fn = smp.losses.DiceLoss(mode='multiclass')
optimizer = torch.optim.Adam(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

"""–®–∞–≥ 7. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""

def train_one_epoch(loader, model, optimizer, loss_fn):
    model.train()
    epoch_loss = 0
    for images, masks in tqdm(loader):
        print(images.dtype, images.shape)
        print(masks.dtype, masks.shape)
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)
        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(loader)


def evaluate(loader, model, loss_fn):
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for images, masks in loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE).long()  # –∏ —Ç—É—Ç —Ç–æ–∂–µ
            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item()
    return val_loss / len(loader)

"""–®–∞–≥ 8. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""

for epoch in range(EPOCHS):
    train_loss = train_one_epoch(train_loader, model, optimizer, loss_fn)
    val_loss = evaluate(val_loader, model, loss_fn)
    scheduler.step()
    print(f"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

# –ü–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
torch.save(model.state_dict(), "esnet_segmentation.pth")

"""# –¢—É—Ç –±—É–¥–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –æ–±–ª–µ–≥—á–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
BATCH_SIZE = 2
NUM_CLASSES = 19
LR = 1e-3
EPOCHS = 3

"""2. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (—Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º)"""

train_transform = A.Compose([
    A.Resize(128, 256),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    ToTensorV2(),
])

val_transform = A.Compose([
    A.Resize(128, 256),
    ToTensorV2(),
])

"""3. Dataset —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –Ω–∞ 300 —Ñ–∞–π–ª–æ–≤"""

class SegmentationDataset(torch.utils.data.Dataset):
    def __init__(self, image_dir, mask_dir, transform):
        self.image_paths = sorted(glob(os.path.join(image_dir, '*.npy')))[:300]
        self.mask_paths = sorted(glob(os.path.join(mask_dir, '*.npy')))[:300]
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = np.load(self.image_paths[idx]).astype(np.uint8)
        mask = np.load(self.mask_paths[idx]).astype(np.uint8)
        mask = np.clip(mask, 0, NUM_CLASSES - 1)  # –≤–∞–∂–Ω–æ
        augmented = self.transform(image=image, mask=mask)
        image = augmented['image']
        mask = augmented['mask']
        return image.float(), mask.long()

train_dataset = SegmentationDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, train_transform)
val_dataset = SegmentationDataset(VAL_IMG_DIR, VAL_MASK_DIR, val_transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2)

model = smp.FPN(encoder_name="resnet18", classes=NUM_CLASSES, activation=None).to(DEVICE)
loss_fn = smp.losses.DiceLoss(mode='multiclass')
optimizer = torch.optim.Adam(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

def train_one_epoch(loader, model, optimizer, loss_fn):
    model.train()
    epoch_loss = 0
    for i, (images, masks) in enumerate(tqdm(loader)):
        images, masks = images.to(DEVICE), masks.to(DEVICE)
        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        if i % 50 == 0:
            print(f"[Batch {i}/{len(loader)}] Loss: {loss.item():.4f}")
    return epoch_loss / len(loader)

def evaluate(loader, model, loss_fn):
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for images, masks in loader:
            images, masks = images.to(DEVICE), masks.to(DEVICE)
            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item()
    return val_loss / len(loader)

for epoch in range(EPOCHS):
    train_loss = train_one_epoch(train_loader, model, optimizer, loss_fn)
    val_loss = evaluate(val_loader, model, loss_fn)
    scheduler.step()
    print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

torch.save(model.state_dict(), "segmentation_model.pth")