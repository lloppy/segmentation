{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sruww9GLqzbd",
        "outputId": "1dc33e70-ce1e-42ce-a9a1-8cc070745d8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl3Jrx4oLcNP",
        "outputId": "2bcaa9d2-2902-4371-a007-f5331e8d6987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "The folder you are executing pip from can no longer be found.\n"
          ]
        }
      ],
      "source": [
        "# Установка зависимостей\n",
        "!pip install -q segmentation-models-pytorch\n",
        "!pip install -q albumentations\n",
        "!pip install -q pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dNj4xQALd3D"
      },
      "outputs": [],
      "source": [
        "# Импорты\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as T\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX7-VFbtLffj"
      },
      "outputs": [],
      "source": [
        "# Конфигурация\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "BATCH_SIZE = 4\n",
        "NUM_CLASSES = 19  # для Cityscapes (18 + 1 кастомный)\n",
        "LR = 1e-3\n",
        "EPOCHS = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8E3FyerLg3U"
      },
      "outputs": [],
      "source": [
        "# Пути к данным\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/data/train/images\"\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/data/train/masks\"\n",
        "VAL_IMG_DIR = \"/content/drive/MyDrive/data/val/images\"\n",
        "VAL_MASK_DIR = \"/content/drive/MyDrive/data/val/masks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "N-cjzAkGLaV0",
        "outputId": "38af71da-7d99-4311-ebff-b7e688ce76f0"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-67ee6dcbd98c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSegmentationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_IMG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_MASK_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "# Аугментации\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(256, 512),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 512),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# Класс датасета\n",
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform):\n",
        "        self.image_paths = sorted(glob(os.path.join(image_dir, '*')))\n",
        "        self.mask_paths = sorted(glob(os.path.join(mask_dir, '*')))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(self.mask_paths[idx]))\n",
        "\n",
        "        augmented = self.transform(image=image, mask=mask)\n",
        "        image = augmented['image']\n",
        "        mask = augmented['mask']\n",
        "\n",
        "        return image, mask.long()\n",
        "\n",
        "# Загрузка данных\n",
        "train_dataset = SegmentationDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, train_transform)\n",
        "val_dataset = SegmentationDataset(VAL_IMG_DIR, VAL_MASK_DIR, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Определение модели ESNet (через Smp для упрощения, как ENet или аналог)\n",
        "model = smp.ESPNetV2(classes=NUM_CLASSES, activation=None).to(DEVICE)\n",
        "\n",
        "# Оптимизатор, лосс, метрики\n",
        "loss_fn = smp.losses.DiceLoss(mode='multiclass')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Функция обучения\n",
        "def train_one_epoch(loader, model, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, masks in tqdm(loader):\n",
        "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "# Функция валидации\n",
        "def evaluate(loader, model, loss_fn):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / len(loader)\n",
        "\n",
        "# Обучение\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_one_epoch(train_loader, model, optimizer, loss_fn)\n",
        "    val_loss = evaluate(val_loader, model, loss_fn)\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Сохранение модели\n",
        "torch.save(model.state_dict(), \"esnet_segmentation.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "llkrxJkeaMUL",
        "outputId": "648c0a6d-4d5a-4928-f85f-fa7a706ee225"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Custom TB Handler failed, unregistering\n"
          ]
        }
      ],
      "source": [
        "# Пример кода дообучения ESNet\n",
        "from model import ESNet  # Ваша реализация или сторонняя\n",
        "from dataset import CustomCityscapes  # с кастомным классом \"courier\"\n",
        "import torch, torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Датасеты\n",
        "train_set = CustomCityscapes(split='train')\n",
        "val_set = CustomCityscapes(split='val')\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=4)\n",
        "\n",
        "# Модель\n",
        "model = ESNet(num_classes=20)  # 19 классов + курьер\n",
        "model = model.cuda()\n",
        "\n",
        "# Обучение\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = CrossEntropyLoss(weight=torch.tensor([...]).cuda())  # class weights\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    for imgs, masks in train_loader:\n",
        "        imgs, masks = imgs.cuda(), masks.cuda()\n",
        "        preds = model(imgs)\n",
        "        loss = criterion(preds, masks)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch} complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}